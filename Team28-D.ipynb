{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Εργασία στο Μάθημα Αναγνώριση Προτύπων - Μέρος Δ\n",
    "## Ομάδα 28\n",
    "### Ονοματεπώνυμα Φοιτητών:  Μαχμουτάϊ Έλενα, Τσουκαλά Ναταλία "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, our goal to develop a classification algorithm using the `datasetC.csv` as our training set. With 5000 samples, each boasting 400 features and labeled from 1 to 5, our collective task is to implement and train a classification method.\n",
    "\n",
    "Following the training phase, we will apply our honed model to the unlabeled `datasetCTest.csv` test set. Our output will poduce a numpy vector named `labels28`, encapsulating the predictions generated by our collaborative efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first experiment, we decided to train and evaluated a neural network based on our `datasetC.csv` using `TensorFlow` and `Keras` and our data visualization is accomplised with `Neptune`. \n",
    "\n",
    "The dataset is split into training and testing sets, and the neural network architecture is systematically varied by exploring different sizes for two hidden dense layers with ReLU activation functions. The network is trained using the Adam optimizer and sparse categorical cross-entropy loss for 10 epochs, and its performance is evaluated on the test set. TensorBoard is employed to log the training process and facilitate the analysis of model training and performance metrics. The experiment aims to identify the optimal configuration of hidden layer sizes that maximizes accuracy on the given dataset. The results, including test loss and accuracy, are printed for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neptune\n",
    "# import tensorflow as tf\n",
    "# from keras import layers\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "\n",
    "# # Load the dataset from the specified path\n",
    "# data = pd.read_csv(r\"D:\\projects\\Pattern-Recognition\\datasetC.csv\", header=None)\n",
    "\n",
    "# # Extract feature matrix (X) and target variable (y) from the dataset\n",
    "# layer_sizes = [10, 64, 128, 256]\n",
    "# data = data.values\n",
    "\n",
    "# X = data[:, :-1]\n",
    "# y = data[:, -1]\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Iterate over different layer sizes for the neural network\n",
    "# for layer1_size in layer_sizes:\n",
    "#     for layer2_size in layer_sizes:\n",
    "        \n",
    "#         run = neptune.init_run(\n",
    "#         project=\"elemxm/Pattern-Recognition\",  # replace with your own\n",
    "#         api_token=\"Add your token\",\n",
    "#         )\n",
    "#         run[\"layer1_size\"] = layer1_size\n",
    "#         run[\"layer2_size\"] = layer2_size\n",
    "\n",
    "#         neptune_callback = NeptuneCallback(run=run) \n",
    "        \n",
    "#         # Create a sequential model (feedforward neural network)\n",
    "#         model = tf.keras.models.Sequential()\n",
    "            \n",
    "#         # Flatten layer to transform input data into a 1D array\n",
    "#         model.add(layers.Flatten(input_shape=(400,)))\n",
    "        \n",
    "#         # First dense layer with ReLU activation\n",
    "#         model.add(layers.Dense(layer1_size, activation='relu'))\n",
    "        \n",
    "#         # Second dense layer with ReLU activation\n",
    "#         model.add(layers.Dense(layer2_size, activation='relu'))\n",
    "        \n",
    "#         # Output layer with softmax activation for multiclass classification\n",
    "#         model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#         # Compile the model with Adam optimizer, sparse categorical crossentropy loss, and accuracy metric\n",
    "#         model.compile(optimizer='adam',\n",
    "#                       loss='sparse_categorical_crossentropy',\n",
    "#                       metrics=['accuracy'])\n",
    "        \n",
    "#         # Train the model on the training data for 10 epochs, with 30% validation split and TensorBoard callback\n",
    "#         model.fit(X_train, y_train, epochs=10, validation_split=0.3, callbacks=[neptune_callback])\n",
    "       \n",
    "#         # Evaluate the trained model on the test set\n",
    "#         test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "#         print(f'Test loss: {test_loss}')\n",
    "#         print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "#         run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Neptune we notice that our best results in validation accuracy for our first experiment are the following:\n",
    "\n",
    "```\n",
    "|--Validation accuracy--|--Layer1 size--|--Layer2 size--|\n",
    "|      0.8053           |      128      |       64      |\n",
    "|      0.804            |      128      |       10      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment we tried to determined if by increasing the number of layers in the previous experiment we would notice drastic improvement in our validation accuracy. Thats why we performed our investigation to a 3 Layer Neural Network and since earlier we noticed some overfitting we descided to add a Dropout layer after each one of the Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neptune\n",
    "# import tensorflow as tf\n",
    "# from keras import layers\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "\n",
    "# data = pd.read_csv(r\"D:\\projects\\Pattern-Recognition\\datasetC.csv\", header=None)\n",
    "\n",
    "# layer_sizes = [10, 64, 128, 256]\n",
    "# data = data.values\n",
    "\n",
    "# X = data[:, :-1]\n",
    "# y = data[:, -1]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# for layer1_size in layer_sizes:\n",
    "#     for layer2_size in layer_sizes:\n",
    "#         for layer3_size in layer_sizes:\n",
    "#             run = neptune.init_run(\n",
    "#             project=\"elemxm/Pattern-Recognition\",  # replace with your own\n",
    "#             api_token=\"Add token\",)\n",
    "#             run[\"layer1_size\"] = layer1_size\n",
    "#             run[\"layer2_size\"] = layer2_size\n",
    "#             run[\"layer3_size\"] = layer3_size\n",
    "\n",
    "#             neptune_callback = NeptuneCallback(run=run)\n",
    "\n",
    "#             model = tf.keras.models.Sequential()\n",
    "            \n",
    "#             model.add(layers.Flatten(input_shape=(400,)))\n",
    "#             model.add(layers.Dense(layer1_size, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.5))\n",
    "#             model.add(layers.Dense(layer2_size, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.5))\n",
    "#             model.add(layers.Dense(layer3_size, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.5))\n",
    "#             model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#             model.compile(optimizer='adam',\n",
    "#                           loss='sparse_categorical_crossentropy',\n",
    "#                           metrics=['accuracy'])\n",
    "            \n",
    "#             model.fit(X_train, y_train, epochs=10, validation_split=0.3, callbacks=[neptune_callback])\n",
    "           \n",
    "#             test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "#             print(f'Test loss: {test_loss}')\n",
    "#             print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard we notice that our best results in validation accuracy for our second experience are the following:\n",
    "\n",
    "```\n",
    "|--Validation accuracy--|--Layer1 size--|--Layer2 size--|--Layer3 size--|\n",
    "|       0.8146          |      256      |       256     |      256      |\n",
    "|       0.808           |      256      |       256     |      64       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing our second experiment we notice that the accuracy has not increased much by increasing the number of the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our previous results were pretty much the same we descide to change our layer type and try various different combination including a convolutional hidden layer in case there are filters which maximally respond to a local region of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neptune\n",
    "# import tensorflow as tf\n",
    "# from keras import layers\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "\n",
    "# # Load data from CSV file\n",
    "# data = pd.read_csv(r\"D:\\projects\\Pattern-Recognition\\datasetC.csv\", header=None)\n",
    "\n",
    "# # Preprocess data by converting it to NumPy array\n",
    "# data = data.values\n",
    "# X = data[:, :-1]\n",
    "# y = data[:, -1]\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Define hyperparameters and architecture variations\n",
    "# layer_sizes = [64, 128, 256]\n",
    "\n",
    "# # Experiment loop for different layer sizes\n",
    "# for layer1_size in layer_sizes:\n",
    "#     for layer2_size in layer_sizes:\n",
    "#         for layer3_size in layer_sizes:\n",
    "\n",
    "#             run = neptune.init_run(\n",
    "#             project=\"elemxm/Pattern-Recognition\",  # replace with your own\n",
    "#             api_token=\"Add your token\",)\n",
    "#             run[\"layer1_size\"] = layer1_size\n",
    "#             run[\"layer2_size\"] = layer2_size\n",
    "#             run[\"layer3_size\"] = layer3_size\n",
    "\n",
    "#             neptune_callback = NeptuneCallback(run=run)\n",
    "\n",
    "#             # Build the sequential model\n",
    "#             model = tf.keras.models.Sequential()\n",
    "#             model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(400, 1)))\n",
    "#             model.add(layers.MaxPooling1D(pool_size=2))\n",
    "#             model.add(layers.Flatten())\n",
    "#             model.add(layers.Dense(layer1_size, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.5))\n",
    "#             model.add(layers.Dense(layer2_size, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.5))\n",
    "#             model.add(layers.Dense(layer3_size, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.5))\n",
    "#             model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#             # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "#             model.compile(optimizer='adam',\n",
    "#                           loss='sparse_categorical_crossentropy',\n",
    "#                           metrics=['accuracy'])\n",
    "\n",
    "#             # Reshape input data for Conv1D layer\n",
    "#             X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "#             X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "#             # Train the model with 10 epochs, 30% validation split, and TensorBoard callback\n",
    "#             model.fit(X_train, y_train, epochs=10, validation_split=0.3, callbacks=[neptune_callback])\n",
    "\n",
    "#             # Evaluate the model on the test set\n",
    "#             test_loss, test_acc = model.evaluate(X_test_reshaped, y_test)\n",
    "            \n",
    "#             # Print results for each experiment\n",
    "#             print(f'Test loss: {test_loss}')\n",
    "#             print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard we notice that our best results in validation accuracy for our third experience are the following:\n",
    "\n",
    "```\n",
    "|--Validation accuracy--|--Layer1 size--|--Layer2 size--|--Layer3 size--|\n",
    "|       0.81            |      128      |       64      |      128      |\n",
    "|       0.806           |      256      |       128     |      256      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there is not much accuracy increase and the results are lower than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our previous investigation it concluded that the model that had the best validation accuracy was from our second experiment with the following characteristics: \n",
    "```\n",
    "|--Validation accuracy--|--Layer1 size--|--Layer2 size--|--Layer3 size--|\n",
    "|       0.814           |      256      |       256     |      256      |\n",
    "```\n",
    "So we decided to perform a huperparameter tuning to our model before first predict our labels from the file `datasetCTest.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neptune\n",
    "# import tensorflow as tf\n",
    "# from keras import layers,regularizers\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "\n",
    "# data = pd.read_csv(r\"D:\\projects\\Pattern-Recognition\\datasetC.csv\", header=None)\n",
    "# data = data.values\n",
    "\n",
    "# X = data[:, :-1]\n",
    "# y = data[:, -1]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# run = neptune.init_run(\n",
    "# project=\"elemxm/Pattern-Recognition\",  # replace with your own\n",
    "# api_token=\"Add your token\",)\n",
    "\n",
    "# neptune_callback = NeptuneCallback(run=run) \n",
    "# final_model = tf.keras.models.Sequential()\n",
    "\n",
    "# final_model.add(layers.Flatten(input_shape=(400,)))\n",
    "# final_model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "# final_model.add(layers.Dropout(0.5))\n",
    "# final_model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "# final_model.add(layers.Dropout(0.5))\n",
    "# final_model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "# final_model.add(layers.Dropout(0.5))\n",
    "# final_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# final_model.compile(optimizer='adam',\n",
    "#                 loss='sparse_categorical_crossentropy',\n",
    "#                 metrics=['accuracy'])\n",
    "\n",
    "# final_model.fit(X_train, y_train, epochs= 7, validation_split=0.3, callbacks=[neptune_callback])\n",
    "\n",
    "# test_loss, test_acc = final_model.evaluate(X_test, y_test)\n",
    "# print(f'Test loss: {test_loss}')\n",
    "# print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that even though the accuracy had a minor increase, we had a bigger increase in the loss. \n",
    "```\n",
    "|--Validation accuracy--|--Validation Loss-- |\n",
    "|       0.817           |      2.217608      |\n",
    "```\n",
    "So we decided for our final model to not use the added huperparameters. We also explored other alternatives but the results were the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our previous investigation it concluded that the model that had the best validation accuracy was from our second experiment with the following characteristics: \n",
    "```\n",
    "|--Validation accuracy--|--Layer1 size--|--Layer2 size--|--Layer3 size--|\n",
    "|       0.814           |      256      |       256     |      256      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/elemxm/Pattern-Recognition/e/PAT-301\n",
      "Epoch 1/10\n",
      "55/55 [==============================] - 2s 12ms/step - loss: 1.9143 - accuracy: 0.2554 - val_loss: 1.4222 - val_accuracy: 0.5627\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 1.3738 - accuracy: 0.4394 - val_loss: 1.0157 - val_accuracy: 0.6373\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 1.0489 - accuracy: 0.5960 - val_loss: 0.7175 - val_accuracy: 0.7640\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.8042 - accuracy: 0.6766 - val_loss: 0.6141 - val_accuracy: 0.7747\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.6592 - accuracy: 0.7446 - val_loss: 0.5595 - val_accuracy: 0.7920\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.5433 - accuracy: 0.8017 - val_loss: 0.5413 - val_accuracy: 0.7947\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4578 - accuracy: 0.8331 - val_loss: 0.5478 - val_accuracy: 0.7867\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3538 - accuracy: 0.8794 - val_loss: 0.5502 - val_accuracy: 0.8120\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3266 - accuracy: 0.8874 - val_loss: 0.5781 - val_accuracy: 0.7987\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.2673 - accuracy: 0.9069 - val_loss: 0.5928 - val_accuracy: 0.7973\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.6207 - accuracy: 0.8068\n",
      "Test loss: 0.6207415461540222\n",
      "Test accuracy: 0.8068000078201294\n"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"D:\\projects\\Pattern-Recognition\\datasetC.csv\", header=None)\n",
    "data = data.values\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "run = neptune.init_run(\n",
    "project=\"elemxm/Pattern-Recognition\",  # replace with your own\n",
    "api_token=\"Add your token\",)\n",
    "\n",
    "neptune_callback = NeptuneCallback(run=run) \n",
    "final_model = tf.keras.models.Sequential()\n",
    "\n",
    "final_model.add(layers.Flatten(input_shape=(400,)))\n",
    "final_model.add(layers.Dense(256, activation='relu'))\n",
    "final_model.add(layers.Dropout(0.5))\n",
    "final_model.add(layers.Dense(256, activation='relu'))\n",
    "final_model.add(layers.Dropout(0.5))\n",
    "final_model.add(layers.Dense(256, activation='relu'))\n",
    "final_model.add(layers.Dropout(0.5))\n",
    "final_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "final_model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "final_model.fit(X_train, y_train, epochs=10, validation_split=0.3, callbacks=[neptune_callback])\n",
    "\n",
    "test_loss, test_acc = final_model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pedict our model and save our data to a npy file named `labels28.npy`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the unlabeled test data\n",
    "test_data = pd.read_csv(r\"D:\\projects\\Pattern-Recognition\\datasetCTest.csv\", header=None)\n",
    "\n",
    "# Apply the trained model to the unlabeled test data\n",
    "labels28 = np.argmax(final_model.predict(test_data.values), axis=1)\n",
    "\n",
    "# Save the labelsX vector in numpy format\n",
    "np.save('labels28.npy', labels28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our labels to ensure that everything works well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 2 4 1 4 1 2 4 3 4 3 5 1 2 1 5 1 3 5 1 5 5 1 1 1 2 1 3 2 2 2 2 2 1 2 2 3\n",
      " 5 3 3 1 3 2 5 4 2 1 5 2 3 3 2 1 2 1 3 2 5 2 4 1 1 3 2 1 3 3 1 3 4 1 4 3 1\n",
      " 1 2 2 3 5 1 2 5 4 4 1 5 3 4 3 2 4 2 3 3 2 4 5 2 4 1 4 1 1 4 3 5 2 4 3 2 5\n",
      " 2 5 4 5 3 2 5 3 4 2 1 1 1 4 5 3 2 5 2 1 5 5 1 4 1 2 1 4 3 4 4 2 4 5 5 2 1\n",
      " 4 2 3 1 2 3 3 3 4 5 4 1 1 1 2 3 4 4 5 2 3 1 1 1 2 2 3 3 4 5 1 1 5 5 3 3 4\n",
      " 3 4 5 1 3 5 3 2 3 3 2 1 2 5 1 3 1 2 1 3 3 5 2 3 5 4 5 2 4 5 1 1 5 3 3 1 1\n",
      " 4 4 4 2 5 3 4 1 3 2 1 2 3 4 4 3 3 4 2 5 1 2 4 5 1 5 3 5 1 1 3 4 4 2 3 2 2\n",
      " 5 1 5 1 5 1 5 4 2 5 1 4 4 1 1 5 2 1 3 4 4 2 5 1 1 2 2 2 5 4 1 4 3 1 1 2 3\n",
      " 3 4 3 3 4 1 1 4 2 4 1 4 1 2 1 4 4 3 4 3 4 4 4 4 5 3 5 4 3 2 1 4 1 5 3 1 3\n",
      " 1 4 2 1 1 3 3 3 4 2 5 1 1 4 2 4 1 4 4 3 2 5 1 4 5 1 1 2 1 2 4 3 3 3 2 4 2\n",
      " 5 1 3 3 5 5 5 4 3 4 3 3 3 3 2 4 5 3 2 5 5 2 4 1 2 3 5 5 3 5 5 1 4 2 3 4 5\n",
      " 4 5 5 1 1 2 3 3 4 3 5 4 1 1 1 5 2 3 4 2 4 4 1 3 1 4 4 5 3 5 5 1 3 3 4 1 1\n",
      " 3 4 5 1 5 5 1 2 4 3 3 5 5 1 4 4 1 4 2 3 1 4 1 4 2 1 4 5 2 4 5 2 1 2 4 3 5\n",
      " 2 5 2 4 4 2 4 3 4 1 2 2 2 1 2 1 5 2 5 3 5 5 4 3 2 3 1 4 2 4 4 3 3 3 2 4 2\n",
      " 1 3 4 4 5 3 3 2 2 3 3 2 5 3 1 3 4 1 1 1 3 5 1 1 1 5 2 1 4 2 1 2 1 3 3 2 1\n",
      " 3 2 3 2 2 1 2 2 3 1 4 3 5 5 4 5 1 4 1 1 5 3 5 1 4 4 2 4 1 4 5 3 1 1 2 4 4\n",
      " 3 3 1 5 5 4 2 3 3 1 3 2 1 4 1 3 3 3 4 5 1 2 5 3 2 1 5 5 1 2 3 1 1 2 2 4 4\n",
      " 2 3 5 1 4 5 1 5 2 1 1 1 5 5 3 3 2 1 2 2 3 1 2 2 3 4 3 3 1 5 1 4 1 4 3 4 5\n",
      " 5 1 1 3 2 3 1 4 1 5 2 1 5 1 5 1 1 2 4 1 5 4 4 2 4 2 1 4 3 5 2 1 5 3 5 1 1\n",
      " 4 3 1 4 3 1 4 2 4 1 1 4 2 5 1 1 3 2 1 2 4 2 3 5 2 4 4 1 3 4 4 4 5 2 3 5 3\n",
      " 3 5 5 5 2 5 2 4 4 4 5 1 2 1 5 5 1 1 3 4 1 1 2 3 2 1 5 3 4 1 3 1 3 3 1 3 1\n",
      " 4 1 2 5 2 1 2 1 4 1 4 1 5 5 1 2 1 2 2 4 2 5 5 1 5 5 2 1 1 1 1 5 3 2 2 1 4\n",
      " 4 4 1 5 1 1 1 5 4 5 2 1 4 3 4 3 4 4 2 5 5 5 1 3 1 2 4 1 5 1 1 1 4 4 4 2 3\n",
      " 5 1 4 2 5 5 3 1 3 4 4 3 1 2 1 2 1 2 1 1 2 1 1 4 1 4 1 2 5 5 3 1 4 1 1 4 2\n",
      " 2 5 1 3 3 4 5 3 5 1 2 2 4 5 5 3 3 1 2 2 3 4 5 4 4 1 1 3 5 2 5 2 5 2 1 2 1\n",
      " 4 3 3 2 1 1 2 5 3 5 2 2 1 4 5 4 1 2 2 1 5 4 4 1 3 5 5 3 4 5 3 1 1 1 2 1 1\n",
      " 4 4 3 3 2 4 5 4 5 1 3 2 1 2 3 3 5 4 1 5 4 4 1 2 3 4 1 5 1 2 5 1 4 5 3 5 4\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# Load the labels28 vector\n",
    "labels28 = np.load('labels28.npy')\n",
    "\n",
    "# Print labels28 vector \n",
    "print(labels28)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
